---
title: "Running fast using paralelism in R or Python"
date: 2019-10-31
tags: [multiprocessing, paralell, R, Python]
---

Have you ever need work in large datasets?

If so, you probably know that every function applied to a dataframe may takes forever. 

In order to solve this issue, I learnt how use all CPUs available in my machine to speed things up! 

In python, there are some caveats before run the script. For example, if you want to run inside jupyter notebook enviroment, we should do the following:

1. Write the function in separate file, and the import as any python library. In this case, I write module called **workers** and the function is called **create_dat**.
2. Should use " if __name__ ==  '__main__': " inside the jupyter notebook in order work properly.

Python code block:

```python
from multiprocessing import Pool
import workers

if __name__ ==  '__main__':
    num_processors = 6
    p=Pool(processes = num_processors)
    output = p.map(workers.create_dat, lst)	
```


On the other hand, in R there are

R code block:

```r

libraty(tidyverse)

```